{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xmlrpclib\n",
    "import time\n",
    "import os.path\n",
    "import errno\n",
    "\n",
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "from multiprocessing import Pool\n",
    "from socket import error as socket_error\n",
    "\n",
    "import cPickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Internal configurations\n",
    "# Do not change this value unless you really know what you are doing.\n",
    "MAX_REQUEST_SIMULTANEOUS = 16\n",
    "CACHE_PATH = \".cache\"\n",
    "STATES_CACHE_FILE = \"css_queries_cache.deepblue\"\n",
    "DEEPBLUE_URL = \"http://deepblue.mpi-inf.mpg.de/xmlrpc\"\n",
    "#DEEPBLUE_URL = \"http://localhost:31415\"\n",
    "USER_KEY = 'anonymous_key'\n",
    "\n",
    "# User parameters (TODO: move to args)\n",
    "GENOME = 'GRCh38'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def request_data(request_id):\n",
    "    (status, info) = SERVER.info(request_id, USER_KEY)\n",
    "    request_status = info[0][\"state\"]\n",
    "\n",
    "    while request_status != \"done\" and request_status != \"failed\":\n",
    "        print info\n",
    "        time.sleep(10)\n",
    "        (status, info) = SERVER.info(request_id, USER_KEY)\n",
    "        request_status = info[0][\"state\"]\n",
    "\n",
    "    print \"Downloading data\"\n",
    "    (status, data) = SERVER.get_request_data(request_id, USER_KEY)\n",
    "    if status != \"okay\":\n",
    "        print data\n",
    "        return None\n",
    "\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def split_file(data):\n",
    "    server_worker = xmlrpclib.Server(DEEPBLUE_URL, allow_none=True)\n",
    "\n",
    "    [css_file, states] = data\n",
    "    result = []\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            _, css_file_query_id = server_worker.select_experiments(\n",
    "                css_file, None, None, None, USER_KEY)\n",
    "            _, css_file_query_cache_id = server_worker.query_cache(\n",
    "                css_file_query_id, True, USER_KEY)\n",
    "            for state in states:\n",
    "                _, css_file_filter_query_id = server_worker.filter_regions(\n",
    "                    css_file_query_cache_id, \"NAME\", \"==\", state, \"string\", USER_KEY)\n",
    "                result.append((state, css_file_filter_query_id))\n",
    "\n",
    "            print css_file\n",
    "            return css_file, result\n",
    "        except socket_error as serr:\n",
    "            print serr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_chromatin_stes(genome, user_key):\n",
    "\n",
    "    status, csss_query_id = SERVER.select_regions(\n",
    "        None, genome, \"Chromatin State Segmentation\",\n",
    "        None, None, None, None, None, None, user_key)\n",
    "    print status, csss_query_id\n",
    "\n",
    "    status, csss_names_request_id = SERVER.distinct_column_values(\n",
    "        csss_query_id, \"NAME\", user_key)\n",
    "    print csss_names_request_id\n",
    "    distinct_values = request_data(csss_names_request_id)\n",
    "    print distinct_values['distinct']\n",
    "\n",
    "    return distinct_values['distinct']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_chromatin_state_files(server, genome, user_key):\n",
    "    cache_file = os.path.join(CACHE_PATH, STATES_CACHE_FILE)\n",
    "\n",
    "    if os.path.exists(cache_file):\n",
    "        _file = open(cache_file, \"r\")\n",
    "        queries = cPickle.load(_file)\n",
    "        return queries\n",
    "    else:\n",
    "        _, css_files_list = server.list_experiments(\n",
    "            genome, \"peaks\", \"Chromatin State Segmentation\", None, None, None, None, user_key)\n",
    "        _, css_files_names = server.extract_names(css_files_list)\n",
    "        states = get_chromatin_stes(genome, user_key)\n",
    "\n",
    "        css_files = []\n",
    "        for css_file in css_files_names:\n",
    "            css_files.append([css_file, states])\n",
    "\n",
    "        pool = Pool(MAX_REQUEST_SIMULTANEOUS)\n",
    "        queries = pool.map(split_file, css_files)\n",
    "        pprint(queries)\n",
    "        datasets = defaultdict(list)\n",
    "\n",
    "        for query in queries:\n",
    "            for state in query[1]:\n",
    "                datasets[query[0]].append([state[1], state[0]])\n",
    "\n",
    "        datasets = dict(datasets)\n",
    "\n",
    "        print datasets\n",
    "\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "        try:\n",
    "            os.makedirs(CACHE_PATH)\n",
    "        except OSError as exc:  # Python >2.5\n",
    "            if exc.errno == errno.EEXIST and os.path.isdir(CACHE_PATH):\n",
    "                pass\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "        _file = open(cache_file, \"w+\")\n",
    "        # print queries\n",
    "        cPickle.dump(datasets, _file)\n",
    "        return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['okay', 'DeepBlue (1.13.7) says hi to anonymous']\n",
      "q978455\n",
      "total of 173 datasets\n"
     ]
    }
   ],
   "source": [
    "SERVER = xmlrpclib.Server(DEEPBLUE_URL, allow_none=True)\n",
    "print SERVER.echo(USER_KEY)\n",
    "\n",
    "# Query data\n",
    "# Select the hypo methylated regions where the AVG methyl level is lower\n",
    "# than 0.0025\n",
    "_, QUERY_ID = SERVER.select_experiments(\n",
    "    \"S00VEQA1.hypo_meth.bs_call.GRCh38.20150707.bed\", None, None, None, USER_KEY)\n",
    "_, QUERY_ID = SERVER.filter_regions(\n",
    "    QUERY_ID, \"AVG_METHYL_LEVEL\", \"<\", \"0.0025\", \"number\", USER_KEY)\n",
    "print QUERY_ID\n",
    "\n",
    "# Universe will be tiling regions of 1000bp\n",
    "_, UNIVERSE_QUERY_ID = SERVER.tiling_regions(1000, \"grch38\", None, USER_KEY)\n",
    "\n",
    "# Datasets will be the chromatin segmentation files divided by segments\n",
    "DATASETS = build_chromatin_state_files(SERVER, GENOME, USER_KEY)\n",
    "print \"total of \" + str(len(DATASETS)) + \" datasets\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing enrich_region_overlap:  r822256\n",
      "Downloading data\n"
     ]
    }
   ],
   "source": [
    "__, ENRICH_REQUEST = SERVER.enrich_region_overlap(QUERY_ID, UNIVERSE_QUERY_ID, DATASETS, \"grch38\", USER_KEY)\n",
    "print \"Processing enrich_region_overlap: \", ENRICH_REQUEST\n",
    "\n",
    "ENRICHMENT_RESULT = request_data(ENRICH_REQUEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('12_Active_TSS_High_Signal_H3K4me3_H3K27Ac', 30.6667),\n",
      " ('12_Active_TSS_High_Signal_H3K4me3_H3K27Ac', 33.3333),\n",
      " ('12_Active_TSS_High_Signal_H3K4me3_H3K27Ac', 37.6667),\n",
      " ('12_Active_TSS_High_Signal_H3K4me3_H3K27Ac', 40.3333),\n",
      " ('12_Active_TSS_High_Signal_H3K4me3_H3K27Ac', 40.6667),\n",
      " ('12_Active_TSS_High_Signal_H3K4me3_H3K27Ac', 42.6667),\n",
      " ('12_Active_TSS_High_Signal_H3K4me3_H3K27Ac', 53.0),\n",
      " ('12_Active_TSS_High_Signal_H3K4me3_H3K27Ac', 54.0),\n",
      " ('11_Active_TSS_High_Signal_H3K4me3_H3K4me1', 55.0),\n",
      " ('12_Active_TSS_High_Signal_H3K4me3_H3K27Ac', 55.0),\n",
      " ('12_Active_TSS_High_Signal_H3K4me3_H3K27Ac', 55.0),\n",
      " ('12_Active_TSS_High_Signal_H3K4me3_H3K27Ac', 55.6667),\n",
      " ('12_Active_TSS_High_Signal_H3K4me3_H3K27Ac', 56.0),\n",
      " ('12_Active_TSS_High_Signal_H3K4me3_H3K27Ac', 56.3333),\n",
      " ('12_Active_TSS_High_Signal_H3K4me3_H3K27Ac', 56.6667),\n",
      " ('12_Active_TSS_High_Signal_H3K4me3_H3K27Ac', 56.6667),\n",
      " ('12_Active_TSS_High_Signal_H3K4me3_H3K27Ac', 56.6667),\n",
      " ('12_Active_TSS_High_Signal_H3K4me3_H3K27Ac', 57.0),\n",
      " ('12_Active_TSS_High_Signal_H3K4me3_H3K27Ac', 58.0),\n",
      " ('12_Active_TSS_High_Signal_H3K4me3_H3K27Ac', 58.0),\n",
      " ('12_Active_TSS_High_Signal_H3K4me3_H3K27Ac', 58.6667),\n",
      " ('12_Active_TSS_High_Signal_H3K4me3_H3K27Ac', 59.0),\n",
      " ('11_Active_TSS_High_Signal_H3K4me3_H3K4me1', 59.6667),\n",
      " ('12_Active_TSS_High_Signal_H3K4me3_H3K27Ac', 60.0),\n",
      " ('12_Active_TSS_High_Signal_H3K4me3_H3K27Ac', 60.6667)]\n"
     ]
    }
   ],
   "source": [
    "state_rank = [(k[\"description\"], k[\"mean_rank\"]) for k in ENRICHMENT_RESULT[\"enrichment\"][\"results\"]][0:25]\n",
    "pprint(state_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
